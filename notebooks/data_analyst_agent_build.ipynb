{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Data Analyst Agent with LangGraph\n",
    "\n",
    "This notebook walks through building a complete Data Analyst Agent step-by-step.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A LangGraph-powered agent that can:\n",
    "- Parse natural language queries about supply chain data\n",
    "- Execute SQL queries on our datasets\n",
    "- Detect anomalies and trends\n",
    "- Generate visualizations\n",
    "- Provide insights and recommendations\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Query ‚Üí Query Parser ‚Üí SQL Executor ‚Üí Visualizer ‚Üí Response Generator\n",
    "                          ‚Üì\n",
    "                    Anomaly Detector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's ensure all required packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in a notebook\n",
    "import sys\n",
    "\n",
    "# Uncomment to install dependencies (if needed)\n",
    "# !pip install langchain langgraph langchain-openai duckdb plotly scikit-learn opik pandas openpyxl python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Annotated, List, Optional, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Opik for observability\n",
    "from opik import track\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data with DuckDB\n",
    "\n",
    "We'll load all 4 datasets into an in-memory DuckDB database for fast SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Initializing DuckDB database...\n",
      "\n",
      "üìä Loading datasets...\n",
      "\n",
      "  ‚úì Loaded DataCoSupplyChainDataset.csv with latin-1 encoding\n",
      "    Shape: (180519, 53)\n",
      "  ‚úì Loaded dynamic_supply_chain_logistics_dataset.csv with utf-8 encoding\n",
      "    Shape: (32065, 26)\n",
      "  ‚úì Loaded supply_chain_data.csv with utf-8 encoding\n",
      "    Shape: (100, 24)\n",
      "  ‚úì Loaded Retail-Supply-Chain-Sales-Dataset.xlsx\n",
      "    Shape: (9994, 23)\n",
      "\n",
      "‚úÖ Loaded 4 tables into DuckDB\n"
     ]
    }
   ],
   "source": [
    "class SupplyChainDBManager:\n",
    "    \"\"\"Manages supply chain data in DuckDB\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = '../data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.conn = duckdb.connect(':memory:')\n",
    "        self.tables = {}\n",
    "        print(\"üì¶ Initializing DuckDB database...\")\n",
    "        \n",
    "    def load_csv_with_encoding(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV with automatic encoding detection\"\"\"\n",
    "        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                print(f\"  ‚úì Loaded {os.path.basename(file_path)} with {encoding} encoding\")\n",
    "                return df\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        raise ValueError(f\"Could not read {file_path} with any encoding\")\n",
    "    \n",
    "    def load_all_datasets(self):\n",
    "        \"\"\"Load all supply chain datasets into DuckDB\"\"\"\n",
    "        print(\"\\nüìä Loading datasets...\\n\")\n",
    "        \n",
    "        # 1. DataCo Supply Chain Dataset\n",
    "        dataco_df = self.load_csv_with_encoding(f\"{self.data_dir}/DataCoSupplyChainDataset.csv\")\n",
    "        # Clean column names\n",
    "        dataco_df.columns = [col.strip().lower().replace(' ', '_').replace('(', '').replace(')', '') for col in dataco_df.columns]\n",
    "        self.conn.register('dataco_supply_chain', dataco_df)\n",
    "        self.tables['dataco_supply_chain'] = dataco_df\n",
    "        print(f\"    Shape: {dataco_df.shape}\")\n",
    "        \n",
    "        # 2. Dynamic Logistics Dataset\n",
    "        logistics_df = self.load_csv_with_encoding(f\"{self.data_dir}/dynamic_supply_chain_logistics_dataset.csv\")\n",
    "        logistics_df.columns = [col.strip().lower().replace(' ', '_') for col in logistics_df.columns]\n",
    "        self.conn.register('logistics', logistics_df)\n",
    "        self.tables['logistics'] = logistics_df\n",
    "        print(f\"    Shape: {logistics_df.shape}\")\n",
    "        \n",
    "        # 3. Supply Chain Data\n",
    "        supply_df = self.load_csv_with_encoding(f\"{self.data_dir}/supply_chain_data.csv\")\n",
    "        supply_df.columns = [col.strip().lower().replace(' ', '_') for col in supply_df.columns]\n",
    "        self.conn.register('supply_chain', supply_df)\n",
    "        self.tables['supply_chain'] = supply_df\n",
    "        print(f\"    Shape: {supply_df.shape}\")\n",
    "        \n",
    "        # 4. Retail Sales Dataset\n",
    "        retail_df = pd.read_excel(f\"{self.data_dir}/Retail-Supply-Chain-Sales-Dataset.xlsx\")\n",
    "        retail_df.columns = [col.strip().lower().replace(' ', '_') for col in retail_df.columns]\n",
    "        self.conn.register('retail_sales', retail_df)\n",
    "        self.tables['retail_sales'] = retail_df\n",
    "        print(f\"  ‚úì Loaded Retail-Supply-Chain-Sales-Dataset.xlsx\")\n",
    "        print(f\"    Shape: {retail_df.shape}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Loaded {len(self.tables)} tables into DuckDB\")\n",
    "        \n",
    "    def get_schema(self) -> str:\n",
    "        \"\"\"Get database schema for LLM context\"\"\"\n",
    "        schema_info = []\n",
    "        for table_name, df in self.tables.items():\n",
    "            schema_info.append(f\"\\nTable: {table_name}\")\n",
    "            schema_info.append(f\"Columns: {', '.join(df.columns.tolist()[:10])}...\")\n",
    "            schema_info.append(f\"Rows: {len(df):,}\")\n",
    "        return \"\\n\".join(schema_info)\n",
    "    \n",
    "    def execute_query(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute SQL query safely\"\"\"\n",
    "        try:\n",
    "            # Add LIMIT if not present\n",
    "            if 'LIMIT' not in query.upper():\n",
    "                query = query.rstrip(';') + ' LIMIT 1000'\n",
    "            result = self.conn.execute(query).df()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå SQL Error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Initialize database\n",
    "db = SupplyChainDBManager()\n",
    "db.load_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Database Schema:\n",
      "\n",
      "\n",
      "Table: dataco_supply_chain\n",
      "Columns: type, days_for_shipping_real, days_for_shipment_scheduled, benefit_per_order, sales_per_customer, delivery_status, late_delivery_risk, category_id, category_name, customer_city...\n",
      "Rows: 180,519\n",
      "\n",
      "Table: logistics\n",
      "Columns: timestamp, vehicle_gps_latitude, vehicle_gps_longitude, fuel_consumption_rate, eta_variation_hours, traffic_congestion_level, warehouse_inventory_level, loading_unloading_time, handling_equipment_availability, order_fulfillment_status...\n",
      "Rows: 32,065\n",
      "\n",
      "Table: supply_chain\n",
      "Columns: product_type, sku, price, availability, number_of_products_sold, revenue_generated, customer_demographics, stock_levels, lead_times, order_quantities...\n",
      "Rows: 100\n",
      "\n",
      "Table: retail_sales\n",
      "Columns: row_id, order_id, order_date, ship_date, ship_mode, customer_id, customer_name, segment, country, city...\n",
      "Rows: 9,994\n",
      "\n",
      "‚úÖ Test query successful! Total orders: 180,519\n"
     ]
    }
   ],
   "source": [
    "# Test the database\n",
    "print(\"\\nüìã Database Schema:\\n\")\n",
    "print(db.get_schema())\n",
    "\n",
    "# Test query\n",
    "test_result = db.execute_query(\"SELECT COUNT(*) as total_orders FROM dataco_supply_chain\")\n",
    "print(f\"\\n‚úÖ Test query successful! Total orders: {test_result['total_orders'].iloc[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Agent State\n",
    "\n",
    "The state flows through all nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ State schema defined\n"
     ]
    }
   ],
   "source": [
    "class DataAnalystState(TypedDict):\n",
    "    \"\"\"State for Data Analyst Agent\"\"\"\n",
    "    # Input\n",
    "    user_query: str\n",
    "    messages: Annotated[List[BaseMessage], \"Conversation messages\"]\n",
    "    \n",
    "    # Query Analysis\n",
    "    query_type: Optional[Literal['sql', 'anomaly', 'trend', 'visualization']]\n",
    "    intent: Optional[str]\n",
    "    \n",
    "    # SQL Execution\n",
    "    sql_query: Optional[str]\n",
    "    query_results: Optional[pd.DataFrame]\n",
    "    \n",
    "    # Analysis\n",
    "    insights: List[str]\n",
    "    anomalies: List[dict]\n",
    "    trends: List[dict]\n",
    "    \n",
    "    # Visualization\n",
    "    charts: List[dict]\n",
    "    \n",
    "    # Output\n",
    "    final_response: Optional[str]\n",
    "    error: Optional[str]\n",
    "\n",
    "print(\"‚úÖ State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize LLM with Opik Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized with Opik tracing\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    callbacks=[OpikTracer()]  # Enable Opik tracing\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized with Opik tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Agent Nodes\n",
    "\n",
    "Each node performs a specific task in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: Query Parser\n",
    "\n",
    "Understands user intent and classifies the query type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Parse user query to determine intent and query type\"\"\"\n",
    "    print(\"\\nüîç Node: Query Parser\")\n",
    "    \n",
    "    user_query = state['user_query']\n",
    "    \n",
    "    prompt = f\"\"\"You are a query classifier for a supply chain analytics system.\n",
    "\n",
    "    User Query: \"{user_query}\"\n",
    "\n",
    "    Classify this query into ONE of these types:\n",
    "    1. 'sql' - Needs database query (e.g., \"What's average sales by region?\")\n",
    "    2. 'anomaly' - Needs anomaly detection (e.g., \"Find unusual deliveries\")\n",
    "    3. 'trend' - Needs trend analysis (e.g., \"Show sales over time\")\n",
    "    4. 'visualization' - Needs specific chart (e.g., \"Plot profit by category\")\n",
    "\n",
    "    Respond with ONLY the type (sql/anomaly/trend/visualization) and a brief intent description.\n",
    "    Format: TYPE | Intent description\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    result = response.content.strip()\n",
    "    \n",
    "    # Parse response\n",
    "    if '|' in result:\n",
    "        query_type, intent = result.split('|', 1)\n",
    "        query_type = query_type.strip().lower()\n",
    "        intent = intent.strip()\n",
    "    else:\n",
    "        query_type = result.lower()\n",
    "        intent = user_query\n",
    "    \n",
    "    print(f\"  Query Type: {query_type}\")\n",
    "    print(f\"  Intent: {intent}\")\n",
    "    \n",
    "    state['query_type'] = query_type\n",
    "    state['intent'] = intent\n",
    "    state['messages'].append(HumanMessage(content=user_query))\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Query Parser node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: SQL Executor\n",
    "\n",
    "Converts natural language to SQL and executes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Generate and execute SQL query\"\"\"\n",
    "    print(\"\\nüíæ Node: SQL Executor\")\n",
    "    \n",
    "    intent = state['intent']\n",
    "    schema = db.get_schema()\n",
    "    \n",
    "    prompt = f\"\"\"You are a SQL expert for a supply chain database.\n",
    "\n",
    "    Database Schema:\n",
    "    {schema}\n",
    "\n",
    "    User wants: {intent}\n",
    "\n",
    "    Generate a valid DuckDB SQL query. Rules:\n",
    "    1. Use only tables in the schema\n",
    "    2. Use clear column aliases\n",
    "    3. Return ONLY the SQL query, no explanations or markdown\n",
    "    4. Do not add LIMIT (it will be added automatically)\n",
    "\n",
    "    SQL Query:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    sql_query = response.content.strip()\n",
    "    \n",
    "    # Clean SQL query\n",
    "    sql_query = sql_query.replace('```sql', '').replace('```', '').strip()\n",
    "    \n",
    "    print(f\"  Generated SQL:\\n  {sql_query}\")\n",
    "    \n",
    "    # Execute query\n",
    "    results = db.execute_query(sql_query)\n",
    "    \n",
    "    if not results.empty:\n",
    "        print(f\"  ‚úì Query returned {len(results)} rows\")\n",
    "        state['sql_query'] = sql_query\n",
    "        state['query_results'] = results\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Query returned no results\")\n",
    "        state['error'] = \"SQL query returned no results\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ SQL Executor node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: Anomaly Detector\n",
    "\n",
    "Detects statistical anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "def detect_anomalies_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Detect anomalies in the data\"\"\"\n",
    "    print(\"\\n‚ö†Ô∏è  Node: Anomaly Detector\")\n",
    "    \n",
    "    # For this demo, we'll detect late deliveries from the dataco dataset\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_city,\n",
    "        days_for_shipping_real,\n",
    "        days_for_shipment_scheduled,\n",
    "        (days_for_shipping_real - days_for_shipment_scheduled) as delay,\n",
    "        delivery_status,\n",
    "        late_delivery_risk\n",
    "    FROM dataco_supply_chain\n",
    "    WHERE days_for_shipping_real IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    results = db.execute_query(query)\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    if not results.empty and 'delay' in results.columns:\n",
    "        # Find delays > 2 days\n",
    "        late_orders = results[results['delay'] > 2]\n",
    "        \n",
    "        if len(late_orders) > 0:\n",
    "            anomalies.append({\n",
    "                'type': 'late_delivery',\n",
    "                'count': len(late_orders),\n",
    "                'avg_delay': float(late_orders['delay'].mean()),\n",
    "                'max_delay': float(late_orders['delay'].max())\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚ö†Ô∏è  Found {len(late_orders)} late deliveries\")\n",
    "            print(f\"     Average delay: {late_orders['delay'].mean():.1f} days\")\n",
    "    \n",
    "    state['anomalies'] = anomalies\n",
    "    state['query_results'] = results\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Anomaly Detector node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: Visualizer\n",
    "\n",
    "Creates charts based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Generate visualizations\"\"\"\n",
    "    print(\"\\nüìä Node: Visualizer\")\n",
    "    \n",
    "    results = state.get('query_results')\n",
    "    charts = []\n",
    "    \n",
    "    if results is not None and not results.empty:\n",
    "        # Determine chart type based on data\n",
    "        if len(results.columns) >= 2:\n",
    "            # Create a simple chart\n",
    "            first_col = results.columns[0]\n",
    "            second_col = results.columns[1]\n",
    "            \n",
    "            # If second column is numeric, create bar chart\n",
    "            if pd.api.types.is_numeric_dtype(results[second_col]):\n",
    "                fig = px.bar(\n",
    "                    results.head(10),\n",
    "                    x=first_col,\n",
    "                    y=second_col,\n",
    "                    title=f\"{second_col} by {first_col}\"\n",
    "                )\n",
    "                charts.append({\n",
    "                    'type': 'bar',\n",
    "                    'title': f\"{second_col} by {first_col}\",\n",
    "                    'figure': fig\n",
    "                })\n",
    "                print(f\"  ‚úì Created bar chart: {second_col} by {first_col}\")\n",
    "    \n",
    "    state['charts'] = charts\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Visualizer node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: Response Generator\n",
    "\n",
    "Synthesizes all findings into a human-readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Generate final response with insights\"\"\"\n",
    "    print(\"\\nüìù Node: Response Generator\")\n",
    "    \n",
    "    query_results = state.get('query_results')\n",
    "    anomalies = state.get('anomalies', [])\n",
    "    sql_query = state.get('sql_query')\n",
    "    charts = state.get('charts', [])\n",
    "    \n",
    "    # Prepare context for LLM\n",
    "    context_parts = []\n",
    "    \n",
    "    if sql_query:\n",
    "        context_parts.append(f\"SQL Query Executed:\\n{sql_query}\")\n",
    "    \n",
    "    if query_results is not None and not query_results.empty:\n",
    "        context_parts.append(f\"\\nQuery Results Summary:\")\n",
    "        context_parts.append(f\"- Rows: {len(query_results)}\")\n",
    "        context_parts.append(f\"- Columns: {', '.join(query_results.columns.tolist())}\")\n",
    "        context_parts.append(f\"\\nFirst few rows:\\n{query_results.head(5).to_string()}\")\n",
    "    \n",
    "    if anomalies:\n",
    "        context_parts.append(f\"\\nAnomalies Detected: {len(anomalies)}\")\n",
    "        for anomaly in anomalies:\n",
    "            context_parts.append(f\"- {anomaly}\")\n",
    "    \n",
    "    if charts:\n",
    "        context_parts.append(f\"\\nVisualizations Created: {len(charts)} chart(s)\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"You are a data analyst providing insights from supply chain data.\n",
    "\n",
    "User Query: {state['user_query']}\n",
    "\n",
    "Analysis Results:\n",
    "{context}\n",
    "\n",
    "Generate a concise analysis report:\n",
    "1. Key Findings (2-3 bullet points)\n",
    "2. Notable Patterns or Trends\n",
    "3. Recommendations (if applicable)\n",
    "\n",
    "Use markdown formatting. Be specific with numbers.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    final_response = response.content\n",
    "    \n",
    "    print(\"  ‚úì Generated insights\")\n",
    "    \n",
    "    state['final_response'] = final_response\n",
    "    state['messages'].append(AIMessage(content=final_response))\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Response Generator node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(state: DataAnalystState) -> str:\n",
    "    \"\"\"Route to appropriate node based on query type\"\"\"\n",
    "    query_type = state.get('query_type', 'sql')\n",
    "    \n",
    "    if query_type == 'anomaly':\n",
    "        return 'detect_anomalies'\n",
    "    elif query_type in ['sql', 'trend']:\n",
    "        return 'execute_sql'\n",
    "    elif query_type == 'visualization':\n",
    "        return 'execute_sql'  # Need data first\n",
    "    else:\n",
    "        return 'execute_sql'\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(DataAnalystState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"parse_query\", parse_query_node)\n",
    "workflow.add_node(\"execute_sql\", execute_sql_node)\n",
    "workflow.add_node(\"detect_anomalies\", detect_anomalies_node)\n",
    "workflow.add_node(\"visualize\", visualize_node)\n",
    "workflow.add_node(\"generate_response\", generate_response_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"parse_query\")\n",
    "\n",
    "# Add conditional routing\n",
    "workflow.add_conditional_edges(\n",
    "    \"parse_query\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"execute_sql\": \"execute_sql\",\n",
    "        \"detect_anomalies\": \"detect_anomalies\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges to visualizer\n",
    "workflow.add_edge(\"execute_sql\", \"visualize\")\n",
    "workflow.add_edge(\"detect_anomalies\", \"visualize\")\n",
    "\n",
    "# Visualizer goes to response generator\n",
    "workflow.add_edge(\"visualize\", \"generate_response\")\n",
    "\n",
    "# Response generator is the end\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph workflow compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create the Agent Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalystAgent:\n",
    "    \"\"\"Data Analyst Agent with Opik tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, graph, db_manager):\n",
    "        self.graph = graph\n",
    "        self.db = db_manager\n",
    "    \n",
    "    @track(name=\"data_analyst_query\", project_name=\"omnisupply-data-analyst\")\n",
    "    def analyze(self, query: str) -> dict:\n",
    "        \"\"\"Analyze a user query\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ü§ñ Data Analyst Agent\")\n",
    "        print(f\"üìù Query: {query}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"user_query\": query,\n",
    "            \"messages\": [],\n",
    "            \"insights\": [],\n",
    "            \"anomalies\": [],\n",
    "            \"trends\": [],\n",
    "            \"charts\": [],\n",
    "            \"query_type\": None,\n",
    "            \"intent\": None,\n",
    "            \"sql_query\": None,\n",
    "            \"query_results\": None,\n",
    "            \"final_response\": None,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        # Run the graph\n",
    "        result = self.graph.invoke(\n",
    "            initial_state,\n",
    "            config={\"callbacks\": [OpikTracer()]}\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize agent\n",
    "data_analyst = DataAnalystAgent(agent, db)\n",
    "print(\"\\n‚úÖ Data Analyst Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Agent!\n",
    "\n",
    "Let's try some example queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Simple SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_analyst.analyze(\"What are the top 5 cities by number of orders?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_response'])\n",
    "\n",
    "if result.get('query_results') is not None:\n",
    "    print(\"\\nüìã Data:\")\n",
    "    display(result['query_results'].head(10))\n",
    "\n",
    "if result.get('charts'):\n",
    "    print(\"\\nüìà Visualizations:\")\n",
    "    for chart in result['charts']:\n",
    "        chart['figure'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_analyst.analyze(\"Find anomalies in delivery times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_response'])\n",
    "\n",
    "if result.get('anomalies'):\n",
    "    print(\"\\n‚ö†Ô∏è  Anomalies Detected:\")\n",
    "    for anomaly in result['anomalies']:\n",
    "        print(f\"  - {anomaly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_analyst.analyze(\"Show me the average sales per customer by shipping mode\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_response'])\n",
    "\n",
    "if result.get('query_results') is not None:\n",
    "    print(\"\\nüìã Data:\")\n",
    "    display(result['query_results'])\n",
    "\n",
    "if result.get('charts'):\n",
    "    for chart in result['charts']:\n",
    "        chart['figure'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Custom Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own query!\n",
    "custom_query = \"What is the average benefit per order for each payment type?\"\n",
    "\n",
    "result = data_analyst.analyze(custom_query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_response'])\n",
    "\n",
    "if result.get('query_results') is not None:\n",
    "    display(result['query_results'])\n",
    "\n",
    "if result.get('charts'):\n",
    "    for chart in result['charts']:\n",
    "        chart['figure'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: View Traces in Opik\n",
    "\n",
    "All your queries are being tracked in Opik! \n",
    "\n",
    "Visit your Comet workspace to see:\n",
    "- Full execution traces\n",
    "- LLM calls and responses\n",
    "- Token usage and costs\n",
    "- Latency metrics\n",
    "\n",
    "üîó [Open Comet Dashboard](https://www.comet.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import Hallucination, AnswerRelevance\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = [\n",
    "    {\n",
    "        'input': 'What are the top 5 cities by order count?',\n",
    "        'expected_query_type': 'sql'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Find late deliveries',\n",
    "        'expected_query_type': 'anomaly'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Show sales trends over time',\n",
    "        'expected_query_type': 'trend'\n",
    "    }\n",
    "]\n",
    "\n",
    "def evaluation_task(item):\n",
    "    \"\"\"Run agent on evaluation item\"\"\"\n",
    "    result = data_analyst.analyze(item['input'])\n",
    "    return {'output': result['final_response']}\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\nüîç Running evaluation...\")\n",
    "evaluation_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[Hallucination(), AnswerRelevance()],\n",
    "    experiment_name=\"data_analyst_evaluation\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete! Check Comet for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "‚úÖ **Data Analyst Agent** with:\n",
    "- Query parsing and intent classification\n",
    "- SQL query generation and execution\n",
    "- Anomaly detection\n",
    "- Automatic visualizations\n",
    "- Natural language insights\n",
    "- Opik observability tracking\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Multi-step reasoning** with LangGraph\n",
    "2. **SQL on DataFrames** with DuckDB\n",
    "3. **Interactive charts** with Plotly\n",
    "4. **Anomaly detection** with statistical methods\n",
    "5. **Full observability** with Opik\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Export to Python modules**: Move code from notebook to `src/` directory\n",
    "2. **Add more nodes**: Forecasting, recommendations, etc.\n",
    "3. **Improve prompts**: Fine-tune for better SQL generation\n",
    "4. **Build other agents**: Risk Agent, Finance Agent, etc.\n",
    "5. **Create multi-agent orchestration**: Connect all agents with LangGraph\n",
    "\n",
    "### Files to Create Next\n",
    "\n",
    "```\n",
    "src/\n",
    "‚îú‚îÄ‚îÄ agents/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ data_analyst/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ agent.py          (from this notebook)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ nodes.py          (all node functions)\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ graph.py          (workflow definition)\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ db_manager.py         (SupplyChainDBManager)\n",
    "‚îî‚îÄ‚îÄ utils/\n",
    "    ‚îî‚îÄ‚îÄ opik_tracker.py       (Opik integration)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OmniSupply-AI-Multi-Agent-Supply-Chain-Intelligence-Platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
