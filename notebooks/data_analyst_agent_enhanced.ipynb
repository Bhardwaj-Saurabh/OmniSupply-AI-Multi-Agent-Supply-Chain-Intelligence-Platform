{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Data Analyst Agent with LangGraph (Enhanced)\n",
    "\n",
    "This notebook builds a robust Data Analyst Agent with:\n",
    "- **Pydantic models** for structured LLM outputs\n",
    "- **Error handling** with retry logic for SQL queries\n",
    "- **Opik observability** for full tracing\n",
    "\n",
    "## Enhancements\n",
    "\n",
    "1. **Query Parser**: Uses Pydantic `QueryClassification` model for type-safe outputs\n",
    "2. **SQL Executor**: Includes retry loop with error feedback to LLM\n",
    "3. **Better validation**: Schema validation before execution\n",
    "4. **Confidence scores**: Track classification confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies (if needed)\n",
    "# !pip install langchain langgraph langchain-openai duckdb plotly scikit-learn opik pandas openpyxl python-dotenv tiktoken pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Annotated, List, Optional, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pydantic for structured outputs\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Opik for observability\n",
    "from opik import track\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Initializing DuckDB database...\n",
      "\n",
      "üìä Loading datasets...\n",
      "\n",
      "  ‚úì Loaded DataCoSupplyChainDataset.csv with latin-1\n",
      "  ‚úì Loaded dynamic_supply_chain_logistics_dataset.csv with utf-8\n",
      "  ‚úì Loaded supply_chain_data.csv with utf-8\n",
      "\n",
      "‚úÖ Loaded 4 tables into DuckDB\n"
     ]
    }
   ],
   "source": [
    "class SupplyChainDBManager:\n",
    "    \"\"\"Manages supply chain data in DuckDB\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = '../data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.conn = duckdb.connect(':memory:')\n",
    "        self.tables = {}\n",
    "        print(\"üì¶ Initializing DuckDB database...\")\n",
    "        \n",
    "    def load_csv_with_encoding(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV with automatic encoding detection\"\"\"\n",
    "        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                print(f\"  ‚úì Loaded {os.path.basename(file_path)} with {encoding}\")\n",
    "                return df\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        raise ValueError(f\"Could not read {file_path}\")\n",
    "    \n",
    "    def load_all_datasets(self):\n",
    "        \"\"\"Load all datasets into DuckDB\"\"\"\n",
    "        print(\"\\nüìä Loading datasets...\\n\")\n",
    "        \n",
    "        # Load all 4 datasets\n",
    "        dataco_df = self.load_csv_with_encoding(f\"{self.data_dir}/DataCoSupplyChainDataset.csv\")\n",
    "        dataco_df.columns = [col.strip().lower().replace(' ', '_').replace('(', '').replace(')', '') for col in dataco_df.columns]\n",
    "        self.conn.register('dataco_supply_chain', dataco_df)\n",
    "        self.tables['dataco_supply_chain'] = dataco_df\n",
    "        \n",
    "        logistics_df = self.load_csv_with_encoding(f\"{self.data_dir}/dynamic_supply_chain_logistics_dataset.csv\")\n",
    "        logistics_df.columns = [col.strip().lower().replace(' ', '_') for col in logistics_df.columns]\n",
    "        self.conn.register('logistics', logistics_df)\n",
    "        self.tables['logistics'] = logistics_df\n",
    "        \n",
    "        supply_df = self.load_csv_with_encoding(f\"{self.data_dir}/supply_chain_data.csv\")\n",
    "        supply_df.columns = [col.strip().lower().replace(' ', '_') for col in supply_df.columns]\n",
    "        self.conn.register('supply_chain', supply_df)\n",
    "        self.tables['supply_chain'] = supply_df\n",
    "        \n",
    "        retail_df = pd.read_excel(f\"{self.data_dir}/Retail-Supply-Chain-Sales-Dataset.xlsx\")\n",
    "        retail_df.columns = [col.strip().lower().replace(' ', '_') for col in retail_df.columns]\n",
    "        self.conn.register('retail_sales', retail_df)\n",
    "        self.tables['retail_sales'] = retail_df\n",
    "        \n",
    "        print(f\"\\n‚úÖ Loaded {len(self.tables)} tables into DuckDB\")\n",
    "    \n",
    "    def get_schema(self) -> str:\n",
    "        \"\"\"Get detailed schema for LLM context\"\"\"\n",
    "        schema_parts = []\n",
    "        for table_name, df in self.tables.items():\n",
    "            schema_parts.append(f\"\\nTable: {table_name}\")\n",
    "            schema_parts.append(f\"Columns: {', '.join(df.columns.tolist())}\")\n",
    "            schema_parts.append(f\"Rows: {len(df):,}\")\n",
    "            # Add sample values for first few columns\n",
    "            schema_parts.append(\"Sample data:\")\n",
    "            for col in df.columns[:5]:\n",
    "                sample_vals = df[col].dropna().head(3).tolist()\n",
    "                schema_parts.append(f\"  {col}: {sample_vals}\")\n",
    "        return \"\\n\".join(schema_parts)\n",
    "    \n",
    "    def execute_query(self, query: str) -> tuple[pd.DataFrame, Optional[str]]:\n",
    "        \"\"\"Execute SQL query and return results + error message if any\"\"\"\n",
    "        try:\n",
    "            # Add LIMIT if not present\n",
    "            if 'LIMIT' not in query.upper():\n",
    "                query = query.rstrip(';') + ' LIMIT 1000'\n",
    "            result = self.conn.execute(query).df()\n",
    "            return result, None\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            return pd.DataFrame(), error_msg\n",
    "\n",
    "# Initialize database\n",
    "db = SupplyChainDBManager()\n",
    "db.load_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Pydantic Models for Structured Outputs\n",
    "\n",
    "These models ensure type-safe, validated responses from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pydantic models defined\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "class QueryEntities(BaseModel):\n",
    "    \"\"\"Extracted entities from query\"\"\"\n",
    "    metrics: List[str] = Field(default_factory=list, description=\"Metrics to measure (e.g., ['sales', 'profit', 'count'])\")\n",
    "    dimensions: List[str] = Field(default_factory=list, description=\"Dimensions to group by (e.g., ['region', 'category'])\")\n",
    "    filters: List[str] = Field(default_factory=list, description=\"Filter conditions (e.g., ['date > 2024-01-01', 'region = East'])\")\n",
    "    time_period: Optional[str] = Field(default=None, description=\"Time period if specified (e.g., 'last 30 days', 'Q1 2024')\")\n",
    "    other_details: Optional[str] = Field(default=None, description=\"Any other relevant details\")\n",
    "\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"Structured output for query parsing\"\"\"\n",
    "    query_type: Literal['sql', 'anomaly', 'trend', 'visualization'] = Field(\n",
    "        description=\"Type of query: sql for database queries, anomaly for outlier detection, trend for time-series, visualization for charts\"\n",
    "    )\n",
    "    intent: str = Field(\n",
    "        description=\"Clear, concise description of what the user wants to accomplish\"\n",
    "    )\n",
    "    entities: QueryEntities = Field(\n",
    "        default_factory=QueryEntities,\n",
    "        description=\"Extracted entities like metrics, dimensions, filters, and time periods\"\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        ge=0.0, le=1.0,\n",
    "        description=\"Confidence score for the classification (0-1)\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Brief explanation of why this classification was chosen\"\n",
    "    )\n",
    "\n",
    "class SQLQueryGeneration(BaseModel):\n",
    "    \"\"\"Structured output for SQL generation\"\"\"\n",
    "    sql_query: str = Field(\n",
    "        description=\"The generated SQL query, properly formatted and valid for DuckDB\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"Brief explanation of what the query does and why it was structured this way\"\n",
    "    )\n",
    "    tables_used: List[str] = Field(\n",
    "        description=\"List of table names used in the query\"\n",
    "    )\n",
    "    expected_columns: List[str] = Field(\n",
    "        description=\"Expected column names in the result set\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Pydantic models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ State schema defined\n"
     ]
    }
   ],
   "source": [
    "class DataAnalystState(TypedDict):\n",
    "    \"\"\"State for Data Analyst Agent\"\"\"\n",
    "    user_query: str\n",
    "    messages: Annotated[List[BaseMessage], \"Conversation messages\"]\n",
    "    \n",
    "    # Query Analysis (now structured)\n",
    "    query_classification: Optional[QueryClassification]\n",
    "    \n",
    "    # SQL Execution\n",
    "    sql_generation: Optional[SQLQueryGeneration]\n",
    "    query_results: Optional[pd.DataFrame]\n",
    "    sql_attempts: int  # Track retry attempts\n",
    "    last_sql_error: Optional[str]  # Last error message\n",
    "    \n",
    "    # Analysis\n",
    "    insights: List[str]\n",
    "    anomalies: List[dict]\n",
    "    charts: List[dict]\n",
    "    \n",
    "    # Output\n",
    "    final_response: Optional[str]\n",
    "    error: Optional[str]\n",
    "\n",
    "print(\"‚úÖ State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize LLM with Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLMs initialized with structured outputs and Opik tracing\n"
     ]
    }
   ],
   "source": [
    "# Base LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    callbacks=[OpikTracer()]\n",
    ")\n",
    "\n",
    "# LLM with structured output for query classification\n",
    "llm_classifier = llm.with_structured_output(QueryClassification)\n",
    "\n",
    "# LLM with structured output for SQL generation\n",
    "llm_sql = llm.with_structured_output(SQLQueryGeneration)\n",
    "\n",
    "print(\"‚úÖ LLMs initialized with structured outputs and Opik tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build Enhanced Agent Nodes\n",
    "\n",
    "### Node 1: Query Parser (Enhanced with Pydantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced Query Parser created\n"
     ]
    }
   ],
   "source": [
    "def parse_query_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Parse user query with structured output\"\"\"\n",
    "    print(\"\\nüîç Node: Query Parser (Structured)\")\n",
    "    \n",
    "    user_query = state['user_query']\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert query classifier for a supply chain analytics system.\n",
    "\n",
    "User Query: \"{user_query}\"\n",
    "\n",
    "Available Data:\n",
    "- dataco_supply_chain: 180K orders with shipping, delivery, customer, product data\n",
    "- logistics: 32K records with GPS, inventory, risk, IoT sensor data\n",
    "- retail_sales: 10K retail orders with profit, returns, regional data\n",
    "- supply_chain: 100 records with manufacturing, procurement data\n",
    "\n",
    "Classify this query and extract relevant entities.\n",
    "\n",
    "Query Types:\n",
    "- 'sql': Needs database query (e.g., \"What's average sales by region?\")\n",
    "- 'anomaly': Needs outlier detection (e.g., \"Find unusual delivery times\")\n",
    "- 'trend': Needs time-series analysis (e.g., \"Show sales trends over time\")\n",
    "- 'visualization': Primary goal is creating a specific chart (e.g., \"Plot profit by category\")\n",
    "\n",
    "Extract entities into structured format:\n",
    "- metrics: List of metrics to measure (e.g., sales, profit, count, average_delay)\n",
    "- dimensions: List of dimensions to group by (e.g., region, category, city)\n",
    "- filters: List of filter conditions (e.g., date ranges, status values)\n",
    "- time_period: Any time period mentioned\n",
    "- other_details: Other relevant information\n",
    "\n",
    "Provide:\n",
    "1. query_type: The classification\n",
    "2. intent: What the user wants to accomplish\n",
    "3. entities: Structured extraction with metrics, dimensions, filters, etc.\n",
    "4. confidence: How confident you are (0-1)\n",
    "5. reasoning: Why you chose this classification\n",
    "\"\"\"\n",
    "    \n",
    "    # Get structured output\n",
    "    classification: QueryClassification = llm_classifier.invoke(prompt)\n",
    "    \n",
    "    print(f\"  Query Type: {classification.query_type}\")\n",
    "    print(f\"  Intent: {classification.intent}\")\n",
    "    print(f\"  Confidence: {classification.confidence:.2f}\")\n",
    "    print(f\"  Entities:\")\n",
    "    print(f\"    - Metrics: {classification.entities.metrics}\")\n",
    "    print(f\"    - Dimensions: {classification.entities.dimensions}\")\n",
    "    print(f\"    - Filters: {classification.entities.filters}\")\n",
    "    if classification.entities.time_period:\n",
    "        print(f\"    - Time Period: {classification.entities.time_period}\")\n",
    "    print(f\"  Reasoning: {classification.reasoning}\")\n",
    "    \n",
    "    state['query_classification'] = classification\n",
    "    state['messages'].append(HumanMessage(content=user_query))\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Enhanced Query Parser created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: SQL Executor (Enhanced with Retry Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced SQL Executor with retry logic created\n"
     ]
    }
   ],
   "source": [
    "def execute_sql_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Generate and execute SQL with error handling and retry\"\"\"\n",
    "    print(\"\\nüíæ Node: SQL Executor (with Retry Logic)\")\n",
    "    \n",
    "    classification = state['query_classification']\n",
    "    max_attempts = 3\n",
    "    attempt = state.get('sql_attempts', 0)\n",
    "    \n",
    "    if attempt >= max_attempts:\n",
    "        print(f\"  ‚ùå Max retry attempts ({max_attempts}) reached\")\n",
    "        state['error'] = f\"Failed to generate valid SQL after {max_attempts} attempts\"\n",
    "        return state\n",
    "    \n",
    "    # Increment attempt counter\n",
    "    state['sql_attempts'] = attempt + 1\n",
    "    \n",
    "    # Get schema\n",
    "    schema = db.get_schema()\n",
    "    \n",
    "    # Build prompt with error feedback if retrying\n",
    "    base_prompt = f\"\"\"You are a SQL expert for a supply chain DuckDB database.\n",
    "\n",
    "Database Schema:\n",
    "{schema}\n",
    "\n",
    "User Intent: {classification.intent}\n",
    "Extracted Entities: {classification.entities}\n",
    "\n",
    "Generate a valid DuckDB SQL query following these rules:\n",
    "1. Use ONLY tables from the schema above\n",
    "2. Use proper column names (check schema carefully)\n",
    "3. Use aliases for better readability\n",
    "4. For aggregations, always include GROUP BY\n",
    "5. Handle NULL values appropriately (use COALESCE or WHERE IS NOT NULL)\n",
    "6. Do NOT add LIMIT (will be added automatically)\n",
    "7. For date filtering, convert string dates using strptime() or TRY_CAST\n",
    "8. Use LOWER() for case-insensitive string comparisons\n",
    "\"\"\"\n",
    "    \n",
    "    # Add error feedback if retrying\n",
    "    if attempt > 0 and state.get('last_sql_error'):\n",
    "        base_prompt += f\"\"\"\n",
    "\n",
    "‚ö†Ô∏è  PREVIOUS ATTEMPT FAILED:\n",
    "Error: {state['last_sql_error']}\n",
    "Previous Query: {state['sql_generation'].sql_query if state.get('sql_generation') else 'N/A'}\n",
    "\n",
    "Fix the error and generate a corrected query. Common fixes:\n",
    "- Check column names match schema exactly (case-sensitive)\n",
    "- Ensure all columns in SELECT are in GROUP BY (if using aggregation)\n",
    "- Use proper DuckDB syntax for dates and strings\n",
    "- Verify table names are spelled correctly\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"  Attempt {state['sql_attempts']}/{max_attempts}\")\n",
    "    \n",
    "    # Generate SQL with structured output\n",
    "    sql_gen: SQLQueryGeneration = llm_sql.invoke(base_prompt)\n",
    "    \n",
    "    print(f\"\\n  Generated SQL:\\n  {sql_gen.sql_query}\")\n",
    "    print(f\"  Explanation: {sql_gen.explanation}\")\n",
    "    print(f\"  Tables Used: {sql_gen.tables_used}\")\n",
    "    print(f\"  Expected Columns: {sql_gen.expected_columns}\")\n",
    "    \n",
    "    # Execute query\n",
    "    results, error = db.execute_query(sql_gen.sql_query)\n",
    "    \n",
    "    if error:\n",
    "        # Query failed - store error and retry\n",
    "        print(f\"  ‚ùå SQL Error: {error}\")\n",
    "        state['last_sql_error'] = error\n",
    "        state['sql_generation'] = sql_gen\n",
    "        \n",
    "        # Recursive call for retry\n",
    "        return execute_sql_node(state)\n",
    "    \n",
    "    # Success!\n",
    "    print(f\"  ‚úÖ Query successful! Returned {len(results)} rows\")\n",
    "    state['sql_generation'] = sql_gen\n",
    "    state['query_results'] = results\n",
    "    state['last_sql_error'] = None\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Enhanced SQL Executor with retry logic created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Anomaly Detector created\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "def detect_anomalies_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Detect anomalies in delivery and logistics data\"\"\"\n",
    "    print(\"\\n‚ö†Ô∏è  Node: Anomaly Detector\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_city,\n",
    "        days_for_shipping_real,\n",
    "        days_for_shipment_scheduled,\n",
    "        (days_for_shipping_real - days_for_shipment_scheduled) as delay,\n",
    "        delivery_status,\n",
    "        late_delivery_risk\n",
    "    FROM dataco_supply_chain\n",
    "    WHERE days_for_shipping_real IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    results, error = db.execute_query(query)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"  ‚ùå Error: {error}\")\n",
    "        state['error'] = error\n",
    "        return state\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    if not results.empty and 'delay' in results.columns:\n",
    "        # Statistical anomaly detection\n",
    "        mean_delay = results['delay'].mean()\n",
    "        std_delay = results['delay'].std()\n",
    "        threshold = mean_delay + (2 * std_delay)\n",
    "        \n",
    "        severe_delays = results[results['delay'] > threshold]\n",
    "        \n",
    "        if len(severe_delays) > 0:\n",
    "            anomalies.append({\n",
    "                'type': 'severe_late_delivery',\n",
    "                'count': len(severe_delays),\n",
    "                'avg_delay': float(severe_delays['delay'].mean()),\n",
    "                'max_delay': float(severe_delays['delay'].max()),\n",
    "                'threshold': float(threshold),\n",
    "                'affected_cities': severe_delays['customer_city'].value_counts().head(5).to_dict()\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚ö†Ô∏è  Found {len(severe_delays)} severe delays (>{threshold:.1f} days)\")\n",
    "            print(f\"     Average delay: {severe_delays['delay'].mean():.1f} days\")\n",
    "            print(f\"     Max delay: {severe_delays['delay'].max():.1f} days\")\n",
    "    \n",
    "    state['anomalies'] = anomalies\n",
    "    state['query_results'] = results\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Anomaly Detector created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visualizer created\n"
     ]
    }
   ],
   "source": [
    "def visualize_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Generate visualizations from query results\"\"\"\n",
    "    print(\"\\nüìä Node: Visualizer\")\n",
    "    \n",
    "    results = state.get('query_results')\n",
    "    charts = []\n",
    "    \n",
    "    if results is not None and not results.empty and len(results.columns) >= 2:\n",
    "        first_col = results.columns[0]\n",
    "        second_col = results.columns[1]\n",
    "        \n",
    "        # Auto-detect chart type\n",
    "        if pd.api.types.is_numeric_dtype(results[second_col]):\n",
    "            # Create bar chart\n",
    "            fig = px.bar(\n",
    "                results.head(15),\n",
    "                x=first_col,\n",
    "                y=second_col,\n",
    "                title=f\"{second_col.replace('_', ' ').title()} by {first_col.replace('_', ' ').title()}\",\n",
    "                labels={first_col: first_col.replace('_', ' ').title(), \n",
    "                        second_col: second_col.replace('_', ' ').title()}\n",
    "            )\n",
    "            fig.update_layout(xaxis_tickangle=-45)\n",
    "            \n",
    "            charts.append({\n",
    "                'type': 'bar',\n",
    "                'title': f\"{second_col} by {first_col}\",\n",
    "                'figure': fig\n",
    "            })\n",
    "            print(f\"  ‚úÖ Created bar chart: {second_col} by {first_col}\")\n",
    "    \n",
    "    state['charts'] = charts\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Visualizer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: Response Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Response Generator created\n"
     ]
    }
   ],
   "source": [
    "def generate_response_node(state: DataAnalystState) -> DataAnalystState:\n",
    "    \"\"\"Generate comprehensive insights\"\"\"\n",
    "    print(\"\\nüìù Node: Response Generator\")\n",
    "    \n",
    "    classification = state.get('query_classification')\n",
    "    sql_gen = state.get('sql_generation')\n",
    "    results = state.get('query_results')\n",
    "    anomalies = state.get('anomalies', [])\n",
    "    charts = state.get('charts', [])\n",
    "    \n",
    "    # Build context\n",
    "    context_parts = []\n",
    "    \n",
    "    if classification:\n",
    "        context_parts.append(f\"Query Type: {classification.query_type}\")\n",
    "        context_parts.append(f\"User Intent: {classification.intent}\")\n",
    "    \n",
    "    if sql_gen:\n",
    "        context_parts.append(f\"\\nSQL Query:\\n{sql_gen.sql_query}\")\n",
    "        context_parts.append(f\"Query Explanation: {sql_gen.explanation}\")\n",
    "    \n",
    "    if results is not None and not results.empty:\n",
    "        context_parts.append(f\"\\nResults: {len(results)} rows, {len(results.columns)} columns\")\n",
    "        context_parts.append(f\"Columns: {', '.join(results.columns.tolist())}\")\n",
    "        context_parts.append(f\"\\nData Preview:\\n{results.head(10).to_string()}\")\n",
    "        \n",
    "        # Add statistics for numeric columns\n",
    "        numeric_cols = results.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            context_parts.append(f\"\\nStatistics:\")\n",
    "            for col in numeric_cols:\n",
    "                context_parts.append(f\"  {col}: mean={results[col].mean():.2f}, min={results[col].min():.2f}, max={results[col].max():.2f}\")\n",
    "    \n",
    "    if anomalies:\n",
    "        context_parts.append(f\"\\n‚ö†Ô∏è  Anomalies Detected:\")\n",
    "        for anom in anomalies:\n",
    "            context_parts.append(f\"  {anom}\")\n",
    "    \n",
    "    if charts:\n",
    "        context_parts.append(f\"\\nüìä {len(charts)} visualization(s) created\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Generate insights\n",
    "    prompt = f\"\"\"You are a senior supply chain data analyst.\n",
    "\n",
    "User Query: {state['user_query']}\n",
    "\n",
    "Analysis Context:\n",
    "{context}\n",
    "\n",
    "Provide a professional analysis report with:\n",
    "\n",
    "## Key Findings\n",
    "- 2-3 specific, data-driven bullet points\n",
    "- Include actual numbers and percentages\n",
    "\n",
    "## Analysis\n",
    "- Interpret the data\n",
    "- Highlight notable patterns or trends\n",
    "- Explain significance of findings\n",
    "\n",
    "## Recommendations\n",
    "- 1-2 actionable recommendations based on the data\n",
    "\n",
    "Use markdown formatting. Be concise but thorough.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    final_response = response.content\n",
    "    \n",
    "    print(\"  ‚úÖ Generated comprehensive insights\")\n",
    "    \n",
    "    state['final_response'] = final_response\n",
    "    state['messages'].append(AIMessage(content=final_response))\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Response Generator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced LangGraph workflow compiled!\n"
     ]
    }
   ],
   "source": [
    "def route_query(state: DataAnalystState) -> str:\n",
    "    \"\"\"Route based on query classification\"\"\"\n",
    "    classification = state.get('query_classification')\n",
    "    if not classification:\n",
    "        return 'execute_sql'\n",
    "    \n",
    "    query_type = classification.query_type\n",
    "    \n",
    "    if query_type == 'anomaly':\n",
    "        return 'detect_anomalies'\n",
    "    else:\n",
    "        return 'execute_sql'\n",
    "\n",
    "# Create graph\n",
    "workflow = StateGraph(DataAnalystState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"parse_query\", parse_query_node)\n",
    "workflow.add_node(\"execute_sql\", execute_sql_node)\n",
    "workflow.add_node(\"detect_anomalies\", detect_anomalies_node)\n",
    "workflow.add_node(\"visualize\", visualize_node)\n",
    "workflow.add_node(\"generate_response\", generate_response_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"parse_query\")\n",
    "\n",
    "# Add routing\n",
    "workflow.add_conditional_edges(\n",
    "    \"parse_query\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"execute_sql\": \"execute_sql\",\n",
    "        \"detect_anomalies\": \"detect_anomalies\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect to visualizer\n",
    "workflow.add_edge(\"execute_sql\", \"visualize\")\n",
    "workflow.add_edge(\"detect_anomalies\", \"visualize\")\n",
    "workflow.add_edge(\"visualize\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile\n",
    "agent = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Enhanced LangGraph workflow compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Agent Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enhanced Data Analyst Agent ready!\n"
     ]
    }
   ],
   "source": [
    "class DataAnalystAgent:\n",
    "    \"\"\"Enhanced Data Analyst Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, graph, db_manager):\n",
    "        self.graph = graph\n",
    "        self.db = db_manager\n",
    "    \n",
    "    @track(name=\"data_analyst_query\", project_name=\"omnisupply-data-analyst\")\n",
    "    def analyze(self, query: str) -> dict:\n",
    "        \"\"\"Analyze a user query with full tracing\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ü§ñ Enhanced Data Analyst Agent\")\n",
    "        print(f\"üìù Query: {query}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"user_query\": query,\n",
    "            \"messages\": [],\n",
    "            \"query_classification\": None,\n",
    "            \"sql_generation\": None,\n",
    "            \"query_results\": None,\n",
    "            \"sql_attempts\": 0,\n",
    "            \"last_sql_error\": None,\n",
    "            \"insights\": [],\n",
    "            \"anomalies\": [],\n",
    "            \"charts\": [],\n",
    "            \"final_response\": None,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        result = self.graph.invoke(\n",
    "            initial_state,\n",
    "            config={\"callbacks\": [OpikTracer()]}\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize agent\n",
    "data_analyst = DataAnalystAgent(agent, db)\n",
    "print(\"\\n‚úÖ Enhanced Data Analyst Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the Enhanced Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"omnisupply-data-analyst\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019ae0b1-45ce-7e16-a9a8-dc61fd85c624&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü§ñ Enhanced Data Analyst Agent\n",
      "üìù Query: What are the top 10 cities with the most orders, and what's their average delivery delay?\n",
      "================================================================================\n",
      "\n",
      "üîç Node: Query Parser (Structured)\n",
      "  Query Type: sql\n",
      "  Intent: Retrieve the top 10 cities with the most orders and their average delivery delay.\n",
      "  Confidence: 0.95\n",
      "  Entities:\n",
      "    - Metrics: ['count', 'average_delivery_delay']\n",
      "    - Dimensions: ['city']\n",
      "    - Filters: []\n",
      "  Reasoning: The query asks for specific metrics (top cities by order count and average delivery delay), which indicates a need for a database query to aggregate and retrieve this information.\n",
      "\n",
      "üíæ Node: SQL Executor (with Retry Logic)\n",
      "  Attempt 1/3\n",
      "\n",
      "  Generated SQL:\n",
      "  SELECT \n",
      "    LOWER(customer_city) AS city, \n",
      "    COUNT(order_id) AS order_count, \n",
      "    AVG(DATEDIFF(shipping_date_dateorders, order_date_dateorders)) AS average_delivery_delay \n",
      "FROM \n",
      "    dataco_supply_chain \n",
      "WHERE \n",
      "    shipping_date_dateorders IS NOT NULL AND order_date_dateorders IS NOT NULL \n",
      "GROUP BY \n",
      "    LOWER(customer_city) \n",
      "ORDER BY \n",
      "    order_count DESC;\n",
      "  Explanation: This query retrieves the top 10 cities with the most orders from the 'dataco_supply_chain' table. It counts the number of orders per city and calculates the average delivery delay by finding the difference between the shipping date and the order date. The results are grouped by city and ordered by the count of orders in descending order. NULL values for shipping and order dates are filtered out to ensure accurate calculations.\n",
      "  Tables Used: ['dataco_supply_chain']\n",
      "  Expected Columns: ['city', 'order_count', 'average_delivery_delay']\n",
      "  ‚ùå SQL Error: Binder Error: No function matches the given name and argument types 'datediff(VARCHAR, VARCHAR)'. You might need to add explicit type casts.\n",
      "\tCandidate functions:\n",
      "\tdatediff(VARCHAR, DATE, DATE) -> BIGINT\n",
      "\tdatediff(VARCHAR, TIME, TIME) -> BIGINT\n",
      "\tdatediff(VARCHAR, TIMESTAMP, TIMESTAMP) -> BIGINT\n",
      "\tdatediff(VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE) -> BIGINT\n",
      "\n",
      "\n",
      "LINE 4:     AVG(DATEDIFF(shipping_date_dateorders, order_date_dateorders...\n",
      "                ^\n",
      "\n",
      "üíæ Node: SQL Executor (with Retry Logic)\n",
      "  Attempt 2/3\n",
      "\n",
      "  Generated SQL:\n",
      "  SELECT \n",
      "    LOWER(customer_city) AS city, \n",
      "    COUNT(order_id) AS order_count, \n",
      "    AVG(DATEDIFF('day', TRY_CAST(shipping_date_dateorders AS TIMESTAMP), TRY_CAST(order_date_dateorders AS TIMESTAMP))) AS average_delivery_delay \n",
      "FROM \n",
      "    dataco_supply_chain \n",
      "WHERE \n",
      "    shipping_date_dateorders IS NOT NULL AND order_date_dateorders IS NOT NULL \n",
      "GROUP BY \n",
      "    LOWER(customer_city) \n",
      "ORDER BY \n",
      "    order_count DESC;\n",
      "  Explanation: This query retrieves the top 10 cities with the most orders and calculates the average delivery delay for each city. It uses the COUNT function to count the number of orders per city and the AVG function to compute the average delivery delay by calculating the difference between the shipping date and the order date. The dates are cast to TIMESTAMP to avoid type errors. The results are grouped by city and ordered by the number of orders in descending order.\n",
      "  Tables Used: ['dataco_supply_chain']\n",
      "  Expected Columns: ['city', 'order_count', 'average_delivery_delay']\n",
      "  ‚úÖ Query successful! Returned 563 rows\n",
      "\n",
      "üìä Node: Visualizer\n",
      "  ‚úÖ Created bar chart: order_count by city\n",
      "\n",
      "üìù Node: Response Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Filtering large LangGraph output (8230 chars) for thread display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Generated comprehensive insights\n",
      "\n",
      "================================================================================\n",
      "üìä RESULTS\n",
      "================================================================================\n",
      "\n",
      "üîç Classification:\n",
      "  Type: sql\n",
      "  Confidence: 95.00%\n",
      "  Entities: metrics=['count', 'average_delivery_delay'] dimensions=['city'] filters=[] time_period=None other_details=None\n",
      "\n",
      "üíæ SQL Generation:\n",
      "  Query: SELECT \n",
      "    LOWER(customer_city) AS city, \n",
      "    COUNT(order_id) AS order_count, \n",
      "    AVG(DATEDIFF('day', TRY_CAST(shipping_date_dateorders AS TIMESTAMP), TRY_CAST(order_date_dateorders AS TIMESTAMP))) AS average_delivery_delay \n",
      "FROM \n",
      "    dataco_supply_chain \n",
      "WHERE \n",
      "    shipping_date_dateorders IS NOT NULL AND order_date_dateorders IS NOT NULL \n",
      "GROUP BY \n",
      "    LOWER(customer_city) \n",
      "ORDER BY \n",
      "    order_count DESC;\n",
      "  Attempts: 2\n",
      "\n",
      "üìù Analysis:\n",
      "\n",
      "# Supply Chain Analysis Report\n",
      "\n",
      "## Key Findings\n",
      "- **Order Volume**: The city of **Caguas** leads significantly with **66,770 orders**, accounting for approximately **88.5%** of the total orders in the dataset, indicating a potential hub for high demand.\n",
      "- **Average Delivery Delay**: The average delivery delay for all cities is currently **NaN**, suggesting that there may be issues with data integrity or missing values in the shipping and order dates, which could hinder performance analysis.\n",
      "- **Diverse Urban Presence**: Other notable cities include **Chicago** with **3,885 orders** and **Los Angeles** with **3,417 orders**, showing a diverse geographic distribution of orders, albeit with significantly lower volumes compared to Caguas.\n",
      "\n",
      "## Analysis\n",
      "The analysis reveals a stark disparity in order volumes among cities, with Caguas dominating the dataset. The absence of valid average delivery delay data (NaN) raises concerns about the reliability of the shipping and order date entries. This could indicate either a data entry issue or a systemic problem in tracking delivery times. \n",
      "\n",
      "The presence of major urban centers like Chicago and Los Angeles in the top ten suggests that while Caguas is a significant player, there is a broader market across various metropolitan areas. However, the lack of average delivery delay data limits our ability to assess the efficiency of the supply chain in these cities.\n",
      "\n",
      "## Recommendations\n",
      "1. **Data Quality Improvement**: Conduct a thorough audit of the shipping and order date fields to identify and rectify any data integrity issues. This will enable accurate calculation of average delivery delays and enhance overall reporting capabilities.\n",
      "   \n",
      "2. **Focus on Caguas**: Given the high order volume in Caguas, consider allocating additional resources or optimizing logistics in this area to further enhance service levels and potentially reduce delivery delays, should the data become available. \n",
      "\n",
      "By addressing these recommendations, the organization can improve its supply chain efficiency and better serve its customer base across various cities.\n",
      "\n",
      "üìã Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>order_count</th>\n",
       "      <th>average_delivery_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caguas</td>\n",
       "      <td>66770</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chicago</td>\n",
       "      <td>3885</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>los angeles</td>\n",
       "      <td>3417</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brooklyn</td>\n",
       "      <td>3412</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york</td>\n",
       "      <td>1816</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>bartlett</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>ponce</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>malden</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>freehold</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>ca</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>563 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            city  order_count  average_delivery_delay\n",
       "0         caguas        66770                     NaN\n",
       "1        chicago         3885                     NaN\n",
       "2    los angeles         3417                     NaN\n",
       "3       brooklyn         3412                     NaN\n",
       "4       new york         1816                     NaN\n",
       "..           ...          ...                     ...\n",
       "558     bartlett           25                     NaN\n",
       "559        ponce           22                     NaN\n",
       "560       malden           22                     NaN\n",
       "561     freehold           13                     NaN\n",
       "562           ca            3                     NaN\n",
       "\n",
       "[563 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "City=%{x}<br>Order Count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "caguas",
          "chicago",
          "los angeles",
          "brooklyn",
          "new york",
          "philadelphia",
          "bronx",
          "san diego",
          "miami",
          "houston",
          "las vegas",
          "dallas",
          "aurora",
          "detroit",
          "phoenix"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "0gQBAC0PAABZDQAAVA0AABgHAAApBgAA3AUAAJ0FAAAiBQAAEQUAAIwEAAAFBAAA9wMAAL4DAACxAwAA",
          "dtype": "i4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Order Count by City"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickangle": -45,
         "title": {
          "text": "City"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Order Count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 1: Complex SQL query\n",
    "result = data_analyst.analyze(\"What are the top 10 cities with the most orders, and what's their average delivery delay?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show classification\n",
    "if result.get('query_classification'):\n",
    "    c = result['query_classification']\n",
    "    print(f\"\\nüîç Classification:\")\n",
    "    print(f\"  Type: {c.query_type}\")\n",
    "    print(f\"  Confidence: {c.confidence:.2%}\")\n",
    "    print(f\"  Entities: {c.entities}\")\n",
    "\n",
    "# Show SQL\n",
    "if result.get('sql_generation'):\n",
    "    sg = result['sql_generation']\n",
    "    print(f\"\\nüíæ SQL Generation:\")\n",
    "    print(f\"  Query: {sg.sql_query}\")\n",
    "    print(f\"  Attempts: {result.get('sql_attempts', 1)}\")\n",
    "\n",
    "# Show insights\n",
    "print(f\"\\nüìù Analysis:\\n\")\n",
    "print(result['final_response'])\n",
    "\n",
    "# Show data\n",
    "if result.get('query_results') is not None:\n",
    "    print(\"\\nüìã Data:\")\n",
    "    display(result['query_results'])\n",
    "\n",
    "# Show charts\n",
    "if result.get('charts'):\n",
    "    for chart in result['charts']:\n",
    "        chart['figure'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü§ñ Enhanced Data Analyst Agent\n",
      "üìù Query: Find deliveries with unusual delays\n",
      "================================================================================\n",
      "\n",
      "üîç Node: Query Parser (Structured)\n",
      "  Query Type: anomaly\n",
      "  Intent: Identify deliveries that are experiencing unusual delays\n",
      "  Confidence: 0.95\n",
      "  Entities:\n",
      "    - Metrics: ['delivery_delay']\n",
      "    - Dimensions: []\n",
      "    - Filters: ['delivery_delay > threshold']\n",
      "  Reasoning: The query specifically asks for 'unusual delays', which indicates a need for anomaly detection to identify outliers in delivery times.\n",
      "\n",
      "‚ö†Ô∏è  Node: Anomaly Detector\n",
      "\n",
      "üìä Node: Visualizer\n",
      "\n",
      "üìù Node: Response Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Filtering large LangGraph output (6614 chars) for thread display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Generated comprehensive insights\n",
      "\n",
      "================================================================================\n",
      "üìä ANOMALY DETECTION RESULTS\n",
      "================================================================================\n",
      "# Analysis Report: Unusual Delivery Delays\n",
      "\n",
      "## Key Findings\n",
      "- **Late Deliveries**: Out of the 1000 deliveries analyzed, 400 (40%) were classified as late deliveries, indicating a significant portion of orders are not meeting scheduled shipping times.\n",
      "- **Average Delay**: The average delay across all deliveries was 0.63 days, with 25% of deliveries experiencing delays greater than 1 day, highlighting a concerning trend in timely order fulfillment.\n",
      "- **High Late Delivery Risk**: 50% of the deliveries were flagged with a late delivery risk of 1, suggesting that half of the orders are at risk of being late, which could impact customer satisfaction and retention.\n",
      "\n",
      "## Analysis\n",
      "The dataset reveals a concerning trend in delivery performance, particularly with a notable percentage of late deliveries. The average days for shipping real (3.89 days) slightly exceeds the average days for shipment scheduled (3.26 days), indicating that many deliveries are not adhering to their planned timelines. \n",
      "\n",
      "- **Delay Distribution**: The delay column shows a range from -2 to 4 days, with a mean delay of 0.63 days. This suggests that while some deliveries are ahead of schedule, a significant number are delayed, with 25% of deliveries experiencing delays greater than 1 day.\n",
      "- **Customer Impact**: The high percentage of late deliveries (40%) and the associated late delivery risk (50%) could lead to dissatisfaction among customers, potentially affecting repeat business and brand reputation.\n",
      "\n",
      "## Recommendations\n",
      "1. **Improve Forecasting and Scheduling**: Implement advanced analytics to better predict shipping times and adjust scheduling accordingly. This could involve analyzing historical data to identify patterns and adjust for peak times or potential disruptions.\n",
      "   \n",
      "2. **Enhance Communication with Customers**: Establish proactive communication strategies to inform customers about potential delays. This could include automated notifications when delays are anticipated, which may help mitigate dissatisfaction and improve customer trust.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Anomaly detection\n",
    "result = data_analyst.analyze(\"Find deliveries with unusual delays\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_response'])\n",
    "\n",
    "if result.get('anomalies'):\n",
    "    print(\"\\n‚ö†Ô∏è  Detailed Anomalies:\")\n",
    "    for anomaly in result['anomalies']:\n",
    "        print(f\"\\n{anomaly}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü§ñ Enhanced Data Analyst Agent\n",
      "üìù Query: Compare average shipping performance across payment types\n",
      "================================================================================\n",
      "\n",
      "üîç Node: Query Parser (Structured)\n",
      "  Query Type: sql\n",
      "  Intent: Compare average shipping performance across different payment types\n",
      "  Confidence: 0.90\n",
      "  Entities:\n",
      "    - Metrics: ['average_shipping_performance']\n",
      "    - Dimensions: ['payment_type']\n",
      "    - Filters: []\n",
      "  Reasoning: The query asks for a comparison of average shipping performance, which implies a need for a database query to calculate averages grouped by payment types. This fits the 'sql' classification as it requires aggregating data.\n",
      "\n",
      "üíæ Node: SQL Executor (with Retry Logic)\n",
      "  Attempt 1/3\n",
      "\n",
      "  Generated SQL:\n",
      "  SELECT ds.type AS payment_type, \n",
      "       AVG(ds.days_for_shipping_real) AS average_shipping_days, \n",
      "       AVG(ds.days_for_shipment_scheduled) AS average_scheduled_days \n",
      "FROM dataco_supply_chain AS ds \n",
      "WHERE ds.type IS NOT NULL \n",
      "GROUP BY ds.type;\n",
      "  Explanation: This query calculates the average shipping performance by payment type from the 'dataco_supply_chain' table. It computes the average of 'days_for_shipping_real' and 'days_for_shipment_scheduled' for each payment type, grouping the results by the 'type' column. The WHERE clause ensures that only non-null payment types are considered.\n",
      "  Tables Used: ['dataco_supply_chain']\n",
      "  Expected Columns: ['payment_type', 'average_shipping_days', 'average_scheduled_days']\n",
      "  ‚úÖ Query successful! Returned 4 rows\n",
      "\n",
      "üìä Node: Visualizer\n",
      "  ‚úÖ Created bar chart: average_shipping_days by payment_type\n",
      "\n",
      "üìù Node: Response Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Filtering large LangGraph output (7332 chars) for thread display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Generated comprehensive insights\n",
      "\n",
      "================================================================================\n",
      "üìä ERROR RECOVERY TEST\n",
      "================================================================================\n",
      "SQL Attempts: 1\n",
      "\n",
      "# Shipping Performance Analysis Report\n",
      "\n",
      "## Key Findings\n",
      "- The average shipping days across all payment types range from **3.47 to 3.53 days**, with **CASH** having the shortest average shipping time at **3.47 days** and **PAYMENT** the longest at **3.53 days**.\n",
      "- The average scheduled shipping days are relatively consistent, ranging from **2.92 to 2.94 days**, with **CASH** again showing the shortest average at **2.92 days**.\n",
      "- The differences in average shipping days between the payment types are minimal, with the longest average shipping time (PAYMENT) being only **0.06 days** longer than the shortest (CASH), indicating a tight performance range across payment methods.\n",
      "\n",
      "## Analysis\n",
      "The data reveals that shipping performance is fairly uniform across different payment types, with average shipping days hovering around **3.5 days**. The slight variations suggest that while payment type may have some influence on shipping performance, other factors such as logistics, warehouse efficiency, or order volume might play a more significant role.\n",
      "\n",
      "Notably, **CASH** transactions not only have the shortest average shipping days but also the lowest average scheduled shipping days, indicating a potentially more efficient processing system for cash payments. In contrast, **PAYMENT** transactions, while still performing well, take slightly longer to ship and schedule, which could warrant further investigation into the underlying processes for this payment type.\n",
      "\n",
      "## Recommendations\n",
      "1. **Investigate Payment Processing Efficiency**: Given that **PAYMENT** transactions have the longest shipping times, it would be beneficial to analyze the processing workflow for these transactions. Identifying bottlenecks could lead to improved shipping performance.\n",
      "   \n",
      "2. **Leverage Cash Payment Insights**: Since **CASH** transactions demonstrate the best shipping performance, consider promoting this payment method to customers. Additionally, explore whether the practices used for cash transactions can be applied to other payment types to enhance overall shipping efficiency.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_type</th>\n",
       "      <th>average_shipping_days</th>\n",
       "      <th>average_scheduled_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASH</td>\n",
       "      <td>3.469260</td>\n",
       "      <td>2.921544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>3.490079</td>\n",
       "      <td>2.932535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>3.528939</td>\n",
       "      <td>2.940012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>3.493174</td>\n",
       "      <td>2.928112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  payment_type  average_shipping_days  average_scheduled_days\n",
       "0         CASH               3.469260                2.921544\n",
       "1        DEBIT               3.490079                2.932535\n",
       "2      PAYMENT               3.528939                2.940012\n",
       "3     TRANSFER               3.493174                2.928112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 3: Test error recovery (intentionally complex query)\n",
    "result = data_analyst.analyze(\"Compare average shipping performance across payment types\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ERROR RECOVERY TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"SQL Attempts: {result.get('sql_attempts', 0)}\")\n",
    "print(f\"\\n{result['final_response']}\")\n",
    "\n",
    "if result.get('query_results') is not None:\n",
    "    display(result['query_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Enhancements Made\n",
    "\n",
    "‚úÖ **Pydantic Models**:\n",
    "- `QueryClassification`: Type-safe query parsing with confidence scores\n",
    "- `SQLQueryGeneration`: Structured SQL generation with metadata\n",
    "\n",
    "‚úÖ **Error Handling**:\n",
    "- SQL retry loop (up to 3 attempts)\n",
    "- Error feedback to LLM for correction\n",
    "- Graceful degradation\n",
    "\n",
    "‚úÖ **Better Observability**:\n",
    "- Track SQL attempts\n",
    "- Log confidence scores\n",
    "- Store error messages\n",
    "\n",
    "‚úÖ **Improved Prompts**:\n",
    "- Detailed schema with samples\n",
    "- Error-specific guidance\n",
    "- DuckDB-specific syntax rules\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Reliability**: Automatic error recovery\n",
    "2. **Transparency**: See classification reasoning\n",
    "3. **Type Safety**: Pydantic validation\n",
    "4. **Debugging**: Full trace of attempts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
